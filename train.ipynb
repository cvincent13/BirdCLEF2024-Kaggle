{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/cedric/.netrc\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    api_key = user_secrets.get_secret(\"WANDB\")\n",
    "    # Login to wandb with the API key\n",
    "    wandb.login(key=api_key)\n",
    "    # Set anonymous mode to None\n",
    "    anonymous = None\n",
    "except:\n",
    "    # If Kaggle secrets are not available, set anonymous mode to 'must'\n",
    "    anonymous = 'must'\n",
    "    # Login to wandb anonymously and relogin if needed\n",
    "    wandb.login(anonymous=anonymous, relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cedric/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import IPython.display as ipd\n",
    "from datetime import datetime\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics.classification import MulticlassAUROC\n",
    "import audiomentations\n",
    "from torch.utils.data import default_collate\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from src.audio_utils import play_audio, plot_specgram, plot_waveform\n",
    "from src.data import AudioDataset, FrequencyMaskingAug, TimeMaskingAug\n",
    "from src.data_utils import get_metadata, get_fold\n",
    "from src.train_utils import FocalLoss, get_cosine_schedule_with_warmup, wandb_init\n",
    "from src.models import BasicClassifier\n",
    "\n",
    "import ast\n",
    "import wandb\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    duration = 10\n",
    "    sample_rate = 32000\n",
    "    target_length = 384\n",
    "    n_mels = 128\n",
    "    n_fft = 2028\n",
    "    window = 2028\n",
    "    audio_len = duration*sample_rate\n",
    "    hop_length = audio_len // (target_length-1)\n",
    "    fmin = 20\n",
    "    fmax = 16000\n",
    "    top_db = 80\n",
    "\n",
    "    n_classes = 182\n",
    "    batch_size = 24\n",
    "    model_name = 'efficientnet_v2_s'\n",
    "    n_folds = 5\n",
    "    upsample_thr = 50\n",
    "    use_class_weights = True\n",
    "\n",
    "    standardize = False\n",
    "    dataset_mean = [-16.8828]\n",
    "    dataset_std = [12.4019]\n",
    "\n",
    "    data_aug = True\n",
    "    cutmix_mixup = True\n",
    "    loss = 'bce'\n",
    "    secondary_labels_weight = 0.3\n",
    "    use_focal = True\n",
    "    focal_gamma = 2\n",
    "    focal_lambda = 1\n",
    "    label_smoothing = 0.05\n",
    "\n",
    "    num_epochs = 10\n",
    "    warmup_epochs = 0.5\n",
    "    lr = 1e-3\n",
    "    start_lr = 0.01 # relative to lr\n",
    "    final_lr = 0.01\n",
    "    weight_decay = 0.0001\n",
    "\n",
    "    wandb = False\n",
    "    competition   = 'birdclef-2024' \n",
    "    _wandb_kernel = 'cvincent13'\n",
    "    date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    run_name = f\"{date}_fold-{0}_dim-{n_mels}x{target_length}_model-{model_name}\"\n",
    "    wandb_group = 'EfficientNetB0|FSR|t=10s|128x384|up_thr=50|cv_filter'\n",
    "\n",
    "metadata = get_metadata(Config.n_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train: 22045, 182 classes |Num Valid: 4892, 182 classes\n"
     ]
    }
   ],
   "source": [
    "fold = 0\n",
    "train_df, valid_df, class_weights = get_fold(metadata, fold, up_thr=Config.upsample_thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transforms and augmentations\n",
    "waveform_transforms = audiomentations.Compose([\n",
    "    audiomentations.Shift(min_shift=-0.5, max_shift=0.5, p=0.5),\n",
    "    audiomentations.SevenBandParametricEQ(min_gain_db=-12., max_gain_db=12., p=0.5),\n",
    "    audiomentations.AirAbsorption(min_temperature=10, max_temperature=20, min_humidity=30, max_humidity=90,\n",
    "                                  min_distance=10, max_distance=100, p=1.), \n",
    "\n",
    "    audiomentations.OneOf([\n",
    "        audiomentations.Gain(min_gain_db=-6., max_gain_db=6., p=1),  # How to handle waveforms out of [-1, 1] ? dont see the issue\n",
    "        audiomentations.GainTransition(min_gain_db=-12., max_gain_db=3., p=1)\n",
    "    ], p=1.),\n",
    "\n",
    "    audiomentations.OneOf([\n",
    "        audiomentations.AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=1.),\n",
    "        audiomentations.AddGaussianSNR(min_snr_db=5., max_snr_db=40., p=1.),\n",
    "        audiomentations.AddColorNoise(min_snr_db=5., max_snr_db=40., min_f_decay=-3.01, max_f_decay=-3.01, p=1.)\n",
    "    ], p=1.),\n",
    "\n",
    "    #audiomentations.AddShortNoises(sounds_path=unlabeled_dir, min_snr_db=3., max_snr_db=30., \n",
    "    #                           noise_rms='relative_to_whole_input',\n",
    "    #                           min_time_between_sounds=2., max_time_between_sounds=8., \n",
    "    #                           noise_transform=audiomentations.PolarityInversion(), p=0.5),\n",
    "    #audiomentations.AddBackgroundNoise(sounds_path=unlabeled_dir, min_snr_db=3., max_snr_db=30., \n",
    "    #                               noise_transform=audiomentations.PolarityInversion(), p=0.5),\n",
    "                                   \n",
    "    audiomentations.LowPassFilter(min_cutoff_freq=750., max_cutoff_freq=7500., min_rolloff=12, max_rolloff=24, p=0.8),\n",
    "    audiomentations.PitchShift(min_semitones=-2.5, max_semitones=2.5, p=0.3)\n",
    "])\n",
    "\n",
    "spec_transforms = nn.Sequential(\n",
    "    FrequencyMaskingAug(0.3, 0.1, Config.n_mels, n_masks=3, mask_mode='mean'),\n",
    "    TimeMaskingAug(0.3, 0.1, Config.target_length, n_masks=3, mask_mode='mean'),\n",
    ")\n",
    "\n",
    "\n",
    "waveform_transforms=None if not Config.data_aug else waveform_transforms\n",
    "spec_transforms=None if not Config.data_aug else spec_transforms\n",
    "\n",
    "\n",
    "train_dataset = AudioDataset(\n",
    "    train_df, \n",
    "    n_classes=Config.n_classes,\n",
    "    duration=Config.duration,\n",
    "    sample_rate=Config.sample_rate,\n",
    "    target_length=Config.target_length,\n",
    "    n_mels=Config.n_mels,\n",
    "    n_fft=Config.n_fft,\n",
    "    window=Config.window,\n",
    "    hop_length=Config.hop_length,\n",
    "    fmin=Config.fmin,\n",
    "    fmax=Config.fmax,\n",
    "    top_db=Config.top_db,\n",
    "    waveform_transforms=waveform_transforms,\n",
    "    spec_transforms=spec_transforms,\n",
    "    standardize=Config.standardize,\n",
    "    mean=Config.dataset_mean,\n",
    "    std=Config.dataset_std,\n",
    "    loss=Config.loss,\n",
    "    secondary_labels_weight=Config.secondary_labels_weight\n",
    "    )\n",
    "val_dataset = AudioDataset(\n",
    "    valid_df, \n",
    "    n_classes=Config.n_classes,\n",
    "    duration=Config.duration,\n",
    "    sample_rate=Config.sample_rate,\n",
    "    target_length=Config.target_length,\n",
    "    n_mels=Config.n_mels,\n",
    "    n_fft=Config.n_fft,\n",
    "    window=Config.window,\n",
    "    hop_length=Config.hop_length,\n",
    "    fmin=Config.fmin,\n",
    "    fmax=Config.fmax,\n",
    "    top_db=Config.top_db,\n",
    "    waveform_transforms=None,\n",
    "    spec_transforms=None,\n",
    "    standardize=Config.standardize,\n",
    "    mean=Config.dataset_mean,\n",
    "    std=Config.dataset_std,\n",
    "    loss=Config.loss,\n",
    "    secondary_labels_weight=Config.secondary_labels_weight\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutmix_or_mixup = v2.RandomApply([\n",
    "    v2.RandomChoice([\n",
    "        v2.CutMix(num_classes=Config.n_classes, alpha=0.5, one_hot_labels=Config.loss=='bce'),\n",
    "        v2.MixUp(num_classes=Config.n_classes, alpha=0.5, one_hot_labels=Config.loss=='bce')\n",
    "    ], p=[0.65, 0.35])\n",
    "], p=0.7)\n",
    "\n",
    "\n",
    "def mix_collate_fn(batch):\n",
    "    return cutmix_or_mixup(*default_collate(batch))\n",
    "\n",
    "collate_fn = mix_collate_fn if Config.cutmix_mixup else None\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=Config.batch_size, shuffle=True, num_workers=6, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=Config.batch_size, shuffle=False, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "model = BasicClassifier(Config.n_classes, Config.model_name).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), weight_decay=Config.weight_decay, lr=Config.lr)\n",
    "spe = len(train_loader)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=spe*Config.warmup_epochs, num_training_steps=spe*Config.num_epochs, \n",
    "                                            start_lr=Config.start_lr, final_lr=Config.final_lr)\n",
    "                                                \n",
    "pos_weight = torch.tensor(class_weights).to(device) if Config.use_class_weights else None\n",
    "if Config.loss == 'crossentropy':\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=Config.label_smoothing, pos_weight=pos_weight)\n",
    "elif Config.loss == 'bce':\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight, weight=None)\n",
    "if Config.use_focal:\n",
    "    focal_criterion = FocalLoss(gamma=Config.focal_gamma, pos_weight=pos_weight)\n",
    "    \n",
    "metric = MulticlassAUROC(num_classes=182, average='macro', thresholds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/919 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss: 0.740:   1%|          | 6/919 [00:09<11:47,  1.29it/s]  "
     ]
    }
   ],
   "source": [
    "if Config.wandb:\n",
    "    run = wandb_init(fold, Config)\n",
    "\n",
    "save_dir = f\"checkpoints/{Config.run_name}\"\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_metrics = {'AUC': [], 'Accuracy': []}\n",
    "\n",
    "for epoch in range(Config.num_epochs):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    train_iter = tqdm(train_loader)\n",
    "    for (batch, labels) in train_iter:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch = batch.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        out = model(batch)\n",
    "        loss = criterion(out, labels) + Config.focal_lambda * focal_criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_iter.set_description(desc=f'train loss: {loss.item():.3f}')\n",
    "        train_loss += loss.item() / len(train_loader)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    val_loss = 0\n",
    "    #val_auc = 0\n",
    "    val_accuracy = 0\n",
    "    model.eval()\n",
    "    val_iter = tqdm(val_loader)\n",
    "    for (batch, labels) in val_iter:\n",
    "        batch = batch.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(batch)\n",
    "            loss = criterion(out, labels) + Config.focal_lambda * focal_criterion(out, labels)\n",
    "\n",
    "        val_loss += loss.item() / len(val_loader)\n",
    "        #val_auc += metric(out, labels) / len(val_loader)\n",
    "        pred = out.argmax(1)\n",
    "        val_accuracy += ((pred == labels.argmax(1)).sum() / len(labels)) / len(val_loader)\n",
    "\n",
    "        val_iter.set_description(desc=f'val loss: {loss.item():.3f}')\n",
    "\n",
    "    val_losses.append(val_loss)\n",
    "    #al_metrics['AUC'].append(val_auc)\n",
    "    val_metrics['Accuracy'].append(val_accuracy)\n",
    "\n",
    "    save_dict = {\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"scheduler\": scheduler.state_dict(),\n",
    "        \"epoch\": epoch+1,\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"val_metrics\": val_metrics\n",
    "    }\n",
    "\n",
    "    torch.save(save_dict, save_dir + \"/checkpoint.pth\")\n",
    "    with open(save_dir + \"logs.txt\", \"w\") as f:\n",
    "        f.write(f\"Epoch {epoch+1}: Train Loss = {train_loss:.3f} |\\\n",
    "          Val Loss = {val_loss:.3f}, Val Accuracy = {val_accuracy:.3f}\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"CONFIG:\")\n",
    "        for k,v in dict(vars(Config)).items():\n",
    "            if '__' not in k:\n",
    "                f.write(\"\\n\")\n",
    "                f.write(f\"{k}: {v}\")\n",
    "\n",
    "\n",
    "    if Config.wandb:\n",
    "        wandb.log({\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "            #\"val_auc\": val_auc,\n",
    "            \"lr\": scheduler.get_last_lr()\n",
    "        })\n",
    "\n",
    "\n",
    "    print(f'Epoch {epoch+1}: Train Loss = {train_loss:.3f} |\\\n",
    "          Val Loss = {val_loss:.3f}, Val Accuracy = {val_accuracy:.3f}')\n",
    "    \n",
    "    \n",
    "if Config.wandb:\n",
    "    #print('# WandB')\n",
    "    #log_wandb(valid_df)\n",
    "    wandb.run.finish()\n",
    "    display(ipd.IFrame(run.url, width=1080, height=720))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
