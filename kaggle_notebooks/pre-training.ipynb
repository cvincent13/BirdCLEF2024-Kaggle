{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":19596,"databundleVersionId":1292430,"sourceType":"competition"},{"sourceId":25954,"databundleVersionId":2091745,"sourceType":"competition"},{"sourceId":33246,"databundleVersionId":3221581,"sourceType":"competition"},{"sourceId":44224,"databundleVersionId":5188730,"sourceType":"competition"},{"sourceId":70203,"databundleVersionId":8068726,"sourceType":"competition"},{"sourceId":1487019,"sourceType":"datasetVersion","datasetId":726237},{"sourceId":1487116,"sourceType":"datasetVersion","datasetId":726312},{"sourceId":2315253,"sourceType":"datasetVersion","datasetId":1389941},{"sourceId":5195317,"sourceType":"datasetVersion","datasetId":3020983},{"sourceId":5821823,"sourceType":"datasetVersion","datasetId":3112969}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\npersonal_token = user_secrets.get_secret(\"github\")\n!git clone https://{personal_token}@github.com/cvincent13/BirdCLEF2024-Kaggle.git","metadata":{"execution":{"iopub.status.busy":"2024-06-06T23:04:15.096016Z","iopub.execute_input":"2024-06-06T23:04:15.096756Z","iopub.status.idle":"2024-06-06T23:04:16.232023Z","shell.execute_reply.started":"2024-06-06T23:04:15.096725Z","shell.execute_reply":"2024-06-06T23:04:16.231088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/working/BirdCLEF2024-Kaggle')","metadata":{"execution":{"iopub.status.busy":"2024-06-06T23:04:16.233945Z","iopub.execute_input":"2024-06-06T23:04:16.234258Z","iopub.status.idle":"2024-06-06T23:04:16.238764Z","shell.execute_reply.started":"2024-06-06T23:04:16.234229Z","shell.execute_reply":"2024-06-06T23:04:16.237882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install audiomentations","metadata":{"execution":{"iopub.status.busy":"2024-06-06T23:04:16.421249Z","iopub.execute_input":"2024-06-06T23:04:16.421608Z","iopub.status.idle":"2024-06-06T23:04:30.287813Z","shell.execute_reply.started":"2024-06-06T23:04:16.421579Z","shell.execute_reply":"2024-06-06T23:04:30.286817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torchaudio\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport IPython.display as ipd\nfrom datetime import datetime\nimport time\nfrom scipy import signal\n\nfrom torch.utils.data import DataLoader\nfrom torchmetrics.classification import MulticlassAccuracy, MultilabelAccuracy\nimport audiomentations\nfrom torch.utils.data import default_collate\nfrom torchvision.transforms import v2\nimport timm\n\nfrom src.audio_utils import play_audio, plot_specgram, plot_waveform\nfrom src.data import AudioDataset, FrequencyMaskingAug, TimeMaskingAug, CutMix, MixUp\nfrom src.data_utils import get_metadata, get_fold, get_metadata_from_csv, get_full_data\nfrom src.utils import upsample_data\nfrom src.train_utils import FocalLoss, BCEFocal2WayLoss, get_cosine_schedule_with_warmup, wandb_init, train_one_epoch, eval_one_epoch\nfrom src.models import AudioMultiHeadGeMSEDClassifier\nfrom src.utils import score_np, roc_auc\n\nimport ast\nimport wandb\nimport yaml\nimport json","metadata":{"execution":{"iopub.status.busy":"2024-06-06T23:04:30.289685Z","iopub.execute_input":"2024-06-06T23:04:30.290012Z","iopub.status.idle":"2024-06-06T23:04:42.645300Z","shell.execute_reply.started":"2024-06-06T23:04:30.289974Z","shell.execute_reply":"2024-06-06T23:04:42.644456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Config","metadata":{}},{"cell_type":"code","source":"class Config:\n    start_idx = 'first'\n    use_1_peak = False\n    peak_filter = 'none'\n    use_peaks = False\n    duration = 5 # 10\n\n    sample_rate = 32000\n    target_length = 384 # 500\n    n_mels = 128\n    n_fft = 1024\n    window = 800 # 160\n    audio_len = duration*sample_rate\n    hop_length = audio_len // (target_length-1)\n    #hop_length = 64\n    fmin = 50\n    fmax = 16000\n    top_db = 80\n    \n    n_classes = 926\n    batch_size = 96\n    Model = AudioMultiHeadGeMSEDClassifier\n    model_name = 'mn30_as'  # dymn10_as mn20_as tf_efficientnetv2_s\n    n_channels = 1\n    upsample_thr = 25\n    use_class_weights = True   # Test\n\n    standardize = False\n    dataset_mean = [-22.9137] #[-16.8828]\n    dataset_std = [11.8739] #[12.4019]\n\n    data_aug = True     # Test     \n    cutmix_mixup = True     # Test\n    loss = 'crossentropy'    # Test ('crossentropy', 'bce')\n    label_smoothing = 0.05  # Only with crossentropy\n    \n    secondary_labels_weight = 0.\n    use_focal = False    # Test (only with bce)\n    use_2wayfocal = False\n    focal_gamma = 2\n    focal_lambda = 1\n\n    num_epochs = 10\n    warmup_epochs = 1\n    lr = 1e-3\n    start_lr = 0.001 # relative to lr\n    final_lr = 0.001\n    weight_decay = 0.0001\n    max_grad_norm = 10\n\n    date = datetime.now().strftime(\"%m-%d_%H-%M\")\n    run_name = f\"{date}_{n_mels}x{target_length}_{model_name}\"\n    exp_name = \"pretraining1\"\n\n    base_dir = '/kaggle/working/BirdCLEF2024-Kaggle/'\n    short_noises = '/kaggle/input/birdclef-2023-additional/esc50/use_label'\n    background_noises = ['/kaggle/input/birdclef2021-background-noise/aicrowd2020_noise_30sec/noise_30sec',\n                         '/kaggle/input/birdclef2021-background-noise/ff1010bird_nocall/nocall'\n                        ]\n\n#train_df, class_weights = get_full_data(Config.base_dir, Config.upsample_thr)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T23:04:42.646824Z","iopub.execute_input":"2024-06-06T23:04:42.647183Z","iopub.status.idle":"2024-06-06T23:04:42.657557Z","shell.execute_reply.started":"2024-06-06T23:04:42.647151Z","shell.execute_reply":"2024-06-06T23:04:42.656317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_names = sorted(set(os.listdir('/kaggle/input/birdclef-2021/train_short_audio') \n                         + os.listdir('/kaggle/input/birdclef-2022/train_audio/') \n                         + os.listdir('/kaggle/input/birdclef-2023/train_audio/') \n                         #+ os.listdir('/kaggle/input/birdsong-recognition/train_audio')\n                         + os.listdir('/kaggle/input/birdclef-2024/train_audio/')\n                         #+ os.listdir('/kaggle/input/xeno-canto-bird-recordings-extended-a-m/A-M')\n                         #+ os.listdir('/kaggle/input/xeno-canto-bird-recordings-extended-n-z/N-Z')\n                        ))\nnum_classes = len(class_names)\nclass_labels = list(range(num_classes))\nlabel2name = dict(zip(class_labels, class_names))\nname2label = {v:k for k,v in label2name.items()}\n\nlen(class_names)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T23:04:42.659933Z","iopub.execute_input":"2024-06-06T23:04:42.660251Z","iopub.status.idle":"2024-06-06T23:04:42.795113Z","shell.execute_reply.started":"2024-06-06T23:04:42.660227Z","shell.execute_reply":"2024-06-06T23:04:42.794008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dir_24 = '/kaggle/input/birdclef-2024'\ndf_24 = pd.read_csv(dir_24 + '/train_metadata.csv')\ndf_24['filepath'] = dir_24 + '/train_audio/' + df_24.filename\ndf_24['target'] = df_24.primary_label.map(name2label)\ndf_24['birdclef'] = '24'\ndf_24['filename'] = df_24.filepath.map(lambda x: x.split('/')[-1])\ndf_24['xc_id'] = df_24.filepath.map(lambda x: x.split('/')[-1].split('.')[0])\n\ndir_23 = '/kaggle/input/birdclef-2023'\ndf_23 = pd.read_csv(dir_23 + '/train_metadata.csv')\ndf_23['filepath'] = dir_23 + '/train_audio/' + df_23.filename\ndf_23['target'] = df_23.primary_label.map(name2label)\ndf_23['birdclef'] = '23'\ndf_23['filename'] = df_23.filepath.map(lambda x: x.split('/')[-1])\ndf_23['xc_id'] = df_23.filepath.map(lambda x: x.split('/')[-1].split('.')[0])\n\ndir_22 = '/kaggle/input/birdclef-2022'\ndf_22 = pd.read_csv(dir_22 + '/train_metadata.csv')\ndf_22['filepath'] = dir_22 + '/train_audio/' + df_22.filename\ndf_22['target'] = df_22.primary_label.map(name2label)\ndf_22['birdclef'] = '22'\ndf_22['filename'] = df_22.filepath.map(lambda x: x.split('/')[-1])\ndf_22['xc_id'] = df_22.filepath.map(lambda x: x.split('/')[-1].split('.')[0])\n\ndir_21 = '/kaggle/input/birdclef-2021'\ndf_21 = pd.read_csv(dir_21 + '/train_metadata.csv')\ndf_21['filepath'] = dir_21 + '/train_short_audio/' + df_21.primary_label + '/' + df_21.filename\ndf_21['target'] = df_21.primary_label.map(name2label)\ndf_21['birdclef'] = '21'\ndf_21['xc_id'] = df_21.filepath.map(lambda x: x.split('/')[-1].split('.')[0])\ncorrupt_paths = ['/kaggle/input/birdclef-2021/train_short_audio/houwre/XC590621.ogg',\n                 '/kaggle/input/birdclef-2021/train_short_audio/cogdov/XC579430.ogg']\ndf_21 = df_21[~df_21.filepath.isin(corrupt_paths)] # remove all zero audios\n\ndir_20 = '/kaggle/input/birdsong-recognition'\ndf_20 = pd.read_csv(dir_20 + '/train.csv')\ndf_20['primary_label'] = df_20['ebird_code']\ndf_20['filepath'] = dir_20 + '/train_audio/' + df_20.primary_label + '/' + df_20.filename\ndf_20['scientific_name'] = df_20['sci_name']\ndf_20['common_name'] = df_20['species']\ndf_20['target'] = df_20.primary_label.map(name2label)\ndf_20['birdclef'] = '20'\n\ndir_xam = '/kaggle/input/xeno-canto-bird-recordings-extended-a-m'\ndf_xam = pd.read_csv(dir_xam + '/train_extended.csv')\ndir_xnz = '/kaggle/input/xeno-canto-bird-recordings-extended-n-z'\ndf_xnz = pd.read_csv(dir_xnz + '/train_extended.csv')\ndf_xam['filepath'] = dir_xam + '/A-M/' + df_xam.ebird_code + '/' + df_xam.filename\ndf_xnz['filepath'] = dir_xnz + '/N-Z/' + df_xnz.ebird_code + '/' + df_xnz.filename\ndf_xc = pd.concat([df_xam, df_xnz], axis=0, ignore_index=True)\ndf_xc['primary_label'] = df_xc['ebird_code']\ndf_xc['scientific_name'] = df_xc['sci_name']\ndf_xc['common_name'] = df_xc['species']\ndf_xc['target'] = df_xc.primary_label.map(name2label)\ndf_xc['birdclef'] = 'xc'\ndf_xc['filename'] = df_xc.filepath.map(lambda x: x.split('/')[-1])\ndf_xc['xc_id'] = df_xc.filepath.map(lambda x: x.split('/')[-1].split('.')[0])\n\n# Merge for pretraining\ndf_pre = pd.concat([df_21, df_22, df_23, df_24], axis=0, ignore_index=True)\ndf_pre['filename'] = df_pre.filepath.map(lambda x: x.split('/')[-1])\ndf_pre['xc_id'] = df_pre.filepath.map(lambda x: x.split('/')[-1].split('.')[0])\nnodup_idx = df_pre[['xc_id','primary_label','author']].drop_duplicates().index\ndf_pre = df_pre.loc[nodup_idx].reset_index(drop=True)\n\n# # Remove duplicates\n#df_pre = df_pre[~df_pre.xc_id.isin(df_24.xc_id)].reset_index(drop=True)\ncorrupt_mp3s = json.load(open('/kaggle/input/birdclef-corrupt-mp3-files-ds/corrupt_mp3_files.json','r'))\ndf_pre = df_pre[~df_pre.filepath.isin(corrupt_mp3s)]\n\ncols = [\"primary_label\", \"filepath\", \"target\"]\npre_train_df = df_pre[cols]\nif Config.upsample_thr is not None:\n    pre_train_df_up = upsample_data(pre_train_df, thr=Config.upsample_thr)\n    pre_train_df = pre_train_df_up.reset_index(drop=True)\n    \nclass_weights = pre_train_df['target'].count()/np.maximum(1, np.bincount(pre_train_df['target']))\nclass_weights = ((class_weights/class_weights.max())**0.5).astype(np.float32)\n\nprint(f\"Num Train: {len(pre_train_df)}, {len(pre_train_df['target'].unique())} classes\")","metadata":{"execution":{"iopub.status.busy":"2024-06-06T23:04:42.796947Z","iopub.execute_input":"2024-06-06T23:04:42.797380Z","iopub.status.idle":"2024-06-06T23:04:47.128008Z","shell.execute_reply.started":"2024-06-06T23:04:42.797344Z","shell.execute_reply":"2024-06-06T23:04:47.126975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not os.path.exists('/kaggle/working/BirdCLEF2024-Kaggle/checkpoints'):\n    os.mkdir('/kaggle/working/BirdCLEF2024-Kaggle/checkpoints')\nif not os.path.exists('/kaggle/working/BirdCLEF2024-Kaggle/config'):\n    os.mkdir('/kaggle/working/BirdCLEF2024-Kaggle/config')","metadata":{"execution":{"iopub.status.busy":"2024-06-06T23:04:47.129535Z","iopub.execute_input":"2024-06-06T23:04:47.129845Z","iopub.status.idle":"2024-06-06T23:04:47.134819Z","shell.execute_reply.started":"2024-06-06T23:04:47.129818Z","shell.execute_reply":"2024-06-06T23:04:47.133847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset","metadata":{}},{"cell_type":"code","source":"# Data transforms and augmentations\nwaveform_transforms = audiomentations.Compose([\n    audiomentations.Shift(min_shift=-0.5, max_shift=0.5, p=0.5),\n    audiomentations.SevenBandParametricEQ(min_gain_db=-12., max_gain_db=12., p=0.2),\n    audiomentations.AirAbsorption(min_temperature=10, max_temperature=20, min_humidity=30, max_humidity=90,\n                                  min_distance=10, max_distance=100, p=0.8), \n\n    audiomentations.OneOf([\n        audiomentations.Gain(min_gain_db=-4., max_gain_db=4., p=1), \n        audiomentations.GainTransition(min_gain_db=-12., max_gain_db=3., p=1)\n    ], p=0.5),\n\n    audiomentations.OneOf([\n        audiomentations.AddGaussianSNR(min_snr_db=5., max_snr_db=40., p=1.),\n        audiomentations.AddColorNoise(min_snr_db=5., max_snr_db=40., min_f_decay=-3.01, max_f_decay=-3.01, p=1.)\n    ], p=0.5),\n\n    audiomentations.AddShortNoises(sounds_path=Config.short_noises, min_snr_db=5., max_snr_db=30., \n                               noise_rms='relative_to_whole_input',\n                               min_time_between_sounds=2., max_time_between_sounds=8., \n                               noise_transform=audiomentations.PolarityInversion(), p=0.4),\n    audiomentations.AddBackgroundNoise(sounds_path=Config.background_noises, min_snr_db=5., max_snr_db=30., \n                                   noise_transform=audiomentations.PolarityInversion(), p=0.4),\n                                   \n    audiomentations.LowPassFilter(min_cutoff_freq=750., max_cutoff_freq=7500., min_rolloff=12, max_rolloff=24, p=0.5),\n    audiomentations.PitchShift(min_semitones=-2.5, max_semitones=2.5, p=0.3)\n])\n\nspec_transforms = nn.Sequential(\n    FrequencyMaskingAug(0.25, 0.1, Config.n_mels, n_masks=3, mask_mode='mean'),\n    TimeMaskingAug(0.25, 0.1, Config.target_length, n_masks=3, mask_mode='mean'),\n)\n\nwaveform_transforms=None if not Config.data_aug else waveform_transforms\nspec_transforms=None if not Config.data_aug else spec_transforms\n\n\ncutmix_or_mixup = v2.RandomApply([\n    v2.RandomChoice([\n        CutMix(num_classes=Config.n_classes, alpha=0.5, one_hot_labels=Config.loss=='bce'),\n        MixUp(num_classes=Config.n_classes, alpha=0.5, one_hot_labels=Config.loss=='bce')\n    ], p=[0.65, 0.35])\n], p=0.5)\n\n\ndef mix_collate_fn(batch):\n    return cutmix_or_mixup(*default_collate(batch))\n\ncollate_fn = mix_collate_fn if Config.cutmix_mixup else None\n","metadata":{"execution":{"iopub.status.busy":"2024-06-06T23:04:47.136056Z","iopub.execute_input":"2024-06-06T23:04:47.136320Z","iopub.status.idle":"2024-06-06T23:04:50.966991Z","shell.execute_reply.started":"2024-06-06T23:04:47.136296Z","shell.execute_reply":"2024-06-06T23:04:50.966184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torchaudio\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms import v2\nfrom scipy import signal\nfrom scipy.ndimage import gaussian_filter1d\nimport librosa\n\ndef trunc_or_pad(waveform, audio_len, start_idx='random'):\n    sig_len = waveform.shape[-1]\n    diff_len = abs(sig_len - audio_len)\n\n    if (sig_len > audio_len):\n        # Truncate the signal to the given length\n        if start_idx == 'random':\n            start_idx = np.random.randint(0, diff_len)\n        else:\n            start_idx = 0\n        waveform = waveform[:, start_idx:start_idx + audio_len]\n    \n    elif (sig_len < audio_len):\n        # Length of padding to add at the beginning and end of the signal\n        pad1 = np.random.randint(0, diff_len)\n        pad2 = diff_len - pad1\n        if isinstance(waveform, torch.Tensor):\n            waveform = nn.functional.pad(waveform, pad=(pad1, pad2), mode='constant', value=0)\n        else:\n            waveform = np.pad(waveform, ((0, 0), (pad1, pad2)), mode='constant', constant_values=0)\n    \n    return waveform\n\ndef create_frames(waveform, duration=5, sr=32000):\n    frame_size = int(duration * sr)\n    surplus = waveform.size(-1)%frame_size\n    if waveform.size(-1) <= surplus:\n        waveform = nn.functional.pad(waveform, pad=(0, frame_size - waveform.size(-1)%frame_size), mode='constant', value=0)\n    elif surplus > 0:\n        waveform = waveform[:, :-surplus]\n    frames = waveform.view(-1, 1, frame_size)\n    return frames\n\n\ndef pcen(x, eps=1E-6, s=0.025, alpha=0.98, delta=2, r=0.5):\n    frames = x.split(1, -2)\n    m_frames = []\n    last_state = None\n    for frame in frames:\n        if last_state is None:\n            last_state = frame\n            m_frames.append(frame)\n            continue\n        m_frame = (1 - s) * last_state + s * frame\n        last_state = m_frame\n        m_frames.append(m_frame)\n    M = torch.cat(m_frames, 1)\n    pcen_ = x.div_(M.add_(eps).pow_(alpha)).add_(delta).pow_(r).sub_(delta ** r)\n    return pcen_, last_state\n\n\ndef find_peak_max(x, filter='savgol'):\n    if filter == 'savgol':\n        smooth_x = signal.savgol_filter(x, window_length=100, polyorder=2)\n    elif filter == 'gaussian':\n        smooth_x = gaussian_filter1d(x, sigma=25)\n    else:\n        smooth_x = x\n    return smooth_x.argmax()\n\ndef window_around_peak(len_x, peak, window_size):\n    half_window = window_size // 2\n    start_index = max(0, peak - half_window)\n    end_index = min(len_x, peak + half_window)\n\n    # Adjust the window if it's too close to the borders\n    if end_index - start_index < window_size:\n        if start_index == 0:\n            end_index = min(len_x, start_index + window_size)\n        elif end_index == len_x:\n            start_index = max(0, end_index - window_size)\n    return start_index, end_index\n\nclass AudioDataset(Dataset):\n    def __init__(\n            self, \n            df, \n            Config,\n            waveform_transforms=None,\n            spec_transforms=None,\n            ):\n        super(AudioDataset, self).__init__()\n        self.df = df\n        self.n_classes = Config.n_classes\n        self.start_idx = Config.start_idx\n        self.duration = Config.duration\n        self.sample_rate = Config.sample_rate\n        self.audio_len = self.duration*self.sample_rate\n        self.target_length = Config.target_length\n        self.n_mels = Config.n_mels\n        self.n_fft = Config.n_fft\n        self.window = Config.window\n        self.hop_length = self.audio_len // (self.target_length-1) if Config.hop_length is None else Config.hop_length\n        self.fmin = Config.fmin\n        self.fmax = Config.fmax\n        self.top_db = Config.top_db\n        self.standardize = Config.standardize\n        self.mean = Config.dataset_mean\n        self.std = Config.dataset_std\n        self.loss = Config.loss\n        self.secondary_labels_weight = Config.secondary_labels_weight\n        self.n_channels = Config.n_channels\n        self.use_1_peak = Config.use_1_peak\n        self.peak_filter = Config.peak_filter\n        self.use_peaks = Config.use_peaks\n\n        self.to_mel_spectrogramn = torchaudio.transforms.MelSpectrogram(self.sample_rate, n_fft=self.n_fft, win_length=self.window,  \n                                                 hop_length=self.hop_length, n_mels=self.n_mels, \n                                                 f_min=self.fmin, f_max=self.fmax)\n\n        self.mel_to_db = nn.Sequential(torchaudio.transforms.AmplitudeToDB(top_db=self.top_db))\n\n        if self.mean is not None and self.std is not None:\n            self.mel_to_db.append(v2.Normalize(mean=self.mean, std=self.std))\n\n        self.waveform_transforms = waveform_transforms\n        self.spec_transforms  = spec_transforms\n        \n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        item = self.df.iloc[idx]\n\n        label = torch.tensor(item['target'])\n        if self.loss == 'bce':\n            label = nn.functional.one_hot(label, num_classes=self.n_classes).float()\n            for l in item['secondary_targets']:\n                if l is not None:\n                    label += nn.functional.one_hot(torch.tensor(l), num_classes=self.n_classes)*self.secondary_labels_weight\n\n        file = item['filepath']\n        waveform, sr = librosa.load(file, sr=self.sample_rate, mono=True)\n        waveform = waveform[None, :]\n        waveform = trunc_or_pad(waveform, self.audio_len, self.start_idx)\n\n        if self.waveform_transforms is not None:\n            waveform = self.waveform_transforms(waveform[0], sr)[None,:] \n            waveform = torch.Tensor(waveform)\n\n        spec = self.to_mel_spectrogramn(waveform)\n\n        if self.use_1_peak:\n            per_frame_energy = spec.sum(dim=1).squeeze().numpy()\n            peak = find_peak_max(per_frame_energy, filter=self.peak_filter)\n            start_index, end_index = window_around_peak(len(per_frame_energy), peak, self.target_length)\n            spec = spec[:,:,start_index:end_index]\n        \n        elif self.use_peaks:\n            per_frame_energy = spec.sum(dim=1).squeeze().numpy()\n            peak1 = find_peak_max(per_frame_energy, filter=self.peak_filter)\n            start_index, end_index = window_around_peak(len(per_frame_energy), peak, self.target_length)\n            spec1 = spec[:,:,start_index:end_index]\n\n\n        spec = self.mel_to_db(spec)\n\n        if self.spec_transforms is not None:\n            spec = self.spec_transforms(spec)\n\n        # Standardize\n        if self.standardize:\n            spec = (spec - spec.mean()) / spec.std()\n\n        # expand to 3 channels for imagenet trained models\n        if self.n_channels > 1:\n            spec = spec.expand(self.n_channels,-1,-1)\n\n        return spec, label","metadata":{"execution":{"iopub.status.busy":"2024-06-06T23:04:50.968467Z","iopub.execute_input":"2024-06-06T23:04:50.968821Z","iopub.status.idle":"2024-06-06T23:04:51.003464Z","shell.execute_reply.started":"2024-06-06T23:04:50.968789Z","shell.execute_reply":"2024-06-06T23:04:51.002290Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = AudioDataset(pre_train_df, Config, waveform_transforms=waveform_transforms, spec_transforms=spec_transforms)\ntrain_loader = DataLoader(train_dataset, batch_size=Config.batch_size, shuffle=True, num_workers=4, collate_fn=collate_fn)\n\nspec, label = next(iter(train_loader))\nplt.figure(figsize=(16,4))\nplt.imshow(spec[0,0], origin='lower', cmap='magma')\nplt.grid(visible=False)\nplt.title('Mel Spectrogram')\nplt.xlabel('Time step')\nplt.ylabel('Mel bin')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-06T23:04:51.004529Z","iopub.execute_input":"2024-06-06T23:04:51.004782Z","iopub.status.idle":"2024-06-06T23:05:37.991579Z","shell.execute_reply.started":"2024-06-06T23:04:51.004760Z","shell.execute_reply":"2024-06-06T23:05:37.990623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda')\n\n# Model and optim\nmodel = Config.Model(Config.n_classes, Config.model_name, n_mels=Config.n_mels).to(device)\nprint(f'Model has {sum([p.numel() for p in model.parameters()]):,} parameters')\noptimizer = torch.optim.Adam(model.parameters(), weight_decay=Config.weight_decay, lr=Config.lr)\nspe = len(train_loader)\nscheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=spe*Config.warmup_epochs, num_training_steps=spe*Config.num_epochs, \n                                            start_lr=Config.start_lr, final_lr=Config.final_lr)\n\n# Losses and metrics\npos_weight = torch.tensor(class_weights).to(device) if Config.use_class_weights else None\nif Config.loss == 'crossentropy':\n    criterion = nn.CrossEntropyLoss(label_smoothing=Config.label_smoothing, weight=pos_weight)\nelif Config.loss == 'bce':\n    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight, weight=None)\n\nfocal_criterion = FocalLoss(gamma=Config.focal_gamma, pos_weight=pos_weight)\nfocal2way_criterion = BCEFocal2WayLoss(gamma=Config.focal_gamma, pos_weight=pos_weight)\n\n# Start training\nstart_time = time.time()\n\nsave_dir = f\"{Config.base_dir}checkpoints/{Config.run_name}\" + f\"_exp-{Config.exp_name}\"\nprint(save_dir)\ntrain_losses = []\ntrain_aucs = []\n\nfor epoch in range(Config.num_epochs):\n    train_loss, gt, preds = train_one_epoch(Config, model, train_loader, device, optimizer, scheduler, \n                                                            criterion, focal_criterion, focal2way_criterion)\n    train_losses.append(train_loss)\n    train_auc = roc_auc(preds, gt)\n    train_aucs.append(train_auc)\n\n    save_dict = {\n        \"model\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict(),\n        \"scheduler\": scheduler.state_dict(),\n        \"epoch\": epoch+1,\n        \"train_losses\": train_losses,\n        \"train_aucs\": train_aucs\n    }\n\n    if not os.path.exists(save_dir):\n        os.mkdir(save_dir)\n    torch.save(save_dict, save_dir + \"/checkpoint.pth\")\n    if epoch+1 % 10 == 0:\n        torch.save(save_dict, save_dir + \"/ep10_checkpoint.pth\")\n    with open(save_dir + \"/config.txt\", \"w\") as f:\n        f.write(\"CONFIG:\")\n        for k,v in dict(vars(Config)).items():\n            if '__' not in k:\n                f.write(\"\\n\")\n                f.write(f\"{k}: {v}\")\n    with open(save_dir + \"/logs.txt\", \"a\") as f:\n        f.write(f\"Epoch {epoch+1}: Train Loss = {train_loss:.3f}, Train ROCAUC = {train_auc:.3f}\\n\")\n\n    print(f'Epoch {epoch+1}: Train Loss = {train_loss:.3f}, Train ROCAUC = {train_auc:.3f}')\n\n\ndef format_duration(seconds):\n    hours, remainder = divmod(seconds, 3600)\n    minutes, seconds = divmod(remainder, 60)\n    return \"{:02}h {:02}min {:02}s\".format(int(hours), int(minutes), int(seconds))\n\nprint(f'Done in {format_duration(time.time() - start_time)}')","metadata":{"execution":{"iopub.status.busy":"2024-06-06T23:05:37.994942Z","iopub.execute_input":"2024-06-06T23:05:37.995492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_dict = {\n    \"model\": model.state_dict(),\n    \"optimizer\": optimizer.state_dict(),\n    \"scheduler\": scheduler.state_dict(),\n    \"epoch\": epoch+1,\n    \"train_losses\": train_losses,\n    \"train_aucs\": train_aucs\n}\n\nif not os.path.exists(save_dir):\n    os.mkdir(save_dir)\ntorch.save(save_dict, save_dir + \"/checkpoint.pth\")","metadata":{"execution":{"iopub.status.busy":"2024-06-06T07:03:23.290030Z","iopub.execute_input":"2024-06-06T07:03:23.290954Z","iopub.status.idle":"2024-06-06T07:03:24.256398Z","shell.execute_reply.started":"2024-06-06T07:03:23.290914Z","shell.execute_reply":"2024-06-06T07:03:24.255352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}